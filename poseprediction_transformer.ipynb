{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qvgxHhCnckGL"
      },
      "outputs": [],
      "source": [
        "# installing dependencies\n",
        "import torchvision.models as models\n",
        "import torch.nn as nn\n",
        "from timm import create_model\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.models as models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torchvision.transforms as transforms\n",
        "from PIL import Image\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "p4iDBKeQcIX1"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ca18f8b2ab1c40f1b0c0525cd90f1968",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\liufy\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liufy\\.cache\\huggingface\\hub\\models--timm--vit_base_patch16_224.augreg2_in21k_ft_in1k. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "c:\\Users\\liufy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# define transformer model's parts being the backbone, transformer encoder, decoder\n",
        "\n",
        "class ViTBackbone(nn.Module):\n",
        "    def __init__(self, model_name=\"vit_base_patch16_224\"):\n",
        "        super().__init__()\n",
        "        # Use a timm model with global_pool disabled\n",
        "        self.vit = create_model(model_name, pretrained=True, num_classes=0, global_pool='')\n",
        "        self.patch_size = 16  # each token corresponds to a 16x16 patch\n",
        "        self.grid_size = 224 // self.patch_size  # e.g., 14\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Use forward_features to obtain the full token sequence\n",
        "        x = self.vit.forward_features(x)\n",
        "        # Remove the class token (first token)\n",
        "        x = x[:, 1:, :]  # now x has shape (B, 196, 768)\n",
        "        B, N, C = x.shape\n",
        "        # Reshape tokens into a spatial grid: (B, C, grid_size, grid_size)\n",
        "        x = x.permute(0, 2, 1).reshape(B, C, self.grid_size, self.grid_size)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Transformer Encoder\n",
        "class Transformer(nn.Module):\n",
        "    # feature_dim is the embed_dim sent in from ViT\n",
        "    # we can also change the number of self-attention heads (num_heads)\n",
        "    # and layers of transformer layers (num_layers)\n",
        "    def __init__(self, feature_dim, num_heads=8, num_layers=6):\n",
        "        super().__init__()\n",
        "        encoder_layers = TransformerEncoderLayer(d_model=feature_dim, nhead=num_heads) # creates a single encoder layer\n",
        "        self.transformer_encoder = TransformerEncoder(encoder_layers, num_layers=num_layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape  # expecting (batch, feature_dim, 14, 14) from ViT, where feature_dim will be 768 from given embed_dim\n",
        "        x = x.flatten(2).permute(2, 0, 1)  # convert (batch, C, H, W) â†’ (seq_len, batch, feature_dim) since that's the order of inputs that TransformerEncoder takes\n",
        "        x = self.transformer_encoder(x)\n",
        "        x = x.permute(1, 2, 0).reshape(B, C, H, W)  # convert back to (batch, C, H, W) so that it remains spatially structured\n",
        "        return x\n",
        "\n",
        "# Heatmap Decoder (Upsamples to 224x224)\n",
        "class HeatmapDecoder(nn.Module):\n",
        "    def __init__(self, feature_dim, num_joints):\n",
        "        super().__init__()\n",
        "        self.upsample1 = nn.ConvTranspose2d(feature_dim, 256, kernel_size=4, stride=2, padding=1)  # upsamples from 14x14 to 28x28\n",
        "        self.upsample2 = nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1)  # same idea, doubles each dimension (now 56)\n",
        "        self.upsample3 = nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1) # (224)\n",
        "        self.upsample4 = nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1) # 224\n",
        "        self.conv_map = nn.Conv2d(32, num_joints, kernel_size=1)  # 1x1 conv to Output 224x224 heatmaps for each joints (from num_joints)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = nn.ReLU()(self.upsample1(x)) # relu to each upsample\n",
        "        x = nn.ReLU()(self.upsample2(x))\n",
        "        x = nn.ReLU()(self.upsample3(x))\n",
        "        x = nn.ReLU()(self.upsample4(x))\n",
        "        x = self.conv_map(x)\n",
        "        return x # heatmaps\n",
        "\n",
        "# defining the full model with all its parts from above\n",
        "backbone = ViTBackbone()\n",
        "transformer = Transformer(feature_dim=768)\n",
        "decoder = HeatmapDecoder(feature_dim=768, num_joints=13)\n",
        "\n",
        "class closedPoseTransformer(nn.Module):\n",
        "    def __init__(self, backbone, transformer, decoder):\n",
        "        super().__init__()\n",
        "        self.backbone = backbone\n",
        "        self.transformer = transformer\n",
        "        self.decoder = decoder\n",
        "    def forward(self, x):\n",
        "        features = self.backbone(x) # (1) extracting features using the ViT\n",
        "        transformed_features = self.transformer(features) # (2) encoder refines features\n",
        "        heatmaps = self.decoder(transformed_features) # (3) upsampled into heatmaps\n",
        "        heatmaps = torch.sigmoid(heatmaps)\n",
        "        return heatmaps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def generate_heatmaps_batch(keypoints, H=224, W=224, sigma=4):\n",
        "    \"\"\"\n",
        "    Vectorized generation of Gaussian heatmaps for a batch of keypoints.\n",
        "    keypoints: tensor of shape [B, num_joints, 2] in pixel space\n",
        "    Returns: tensor of shape [B, num_joints, H, W]\n",
        "    If a keypoint is (0,0) (assumed missing/invisible), its heatmap will be all zeros.\n",
        "    \"\"\"\n",
        "    B, J, _ = keypoints.shape\n",
        "    device = keypoints.device\n",
        "    # Create coordinate grid once.\n",
        "    x_lin = torch.linspace(0, W - 1, W, device=device)\n",
        "    y_lin = torch.linspace(0, H - 1, H, device=device)\n",
        "    y_grid, x_grid = torch.meshgrid(y_lin, x_lin, indexing=\"ij\")  # shape: [H, W]\n",
        "    # Expand grid to match batch & joints.\n",
        "    # Final shapes: [B, J, H, W]\n",
        "    x_grid = x_grid.unsqueeze(0).unsqueeze(0)  # shape [1,1,H,W]\n",
        "    y_grid = y_grid.unsqueeze(0).unsqueeze(0)\n",
        "    \n",
        "    # Expand keypoints from shape [B, J, 2] to [B, J, 1, 1]\n",
        "    kp_exp = keypoints.unsqueeze(-1).unsqueeze(-1)\n",
        "    kp_x = kp_exp[:, :, 0, :, :]  # shape [B, J, 1, 1]\n",
        "    kp_y = kp_exp[:, :, 1, :, :]\n",
        "    \n",
        "    # Compute squared distance from the keypoint location\n",
        "    dist_sq = (x_grid - kp_x)**2 + (y_grid - kp_y)**2\n",
        "    # Compute Gaussian heatmaps\n",
        "    heatmaps = torch.exp(-dist_sq / (2 * sigma**2))\n",
        "    \n",
        "    # If a keypoint is (0,0) (assuming missing) then set its heatmap to zero.\n",
        "    mask_missing = (keypoints.abs().sum(dim=-1) == 0).unsqueeze(-1).unsqueeze(-1)\n",
        "    heatmaps = heatmaps * (1 - mask_missing.float())\n",
        "    return heatmaps\n",
        "\n",
        "\n",
        "\n",
        "# Training and Evaluation Functions\n",
        "class KeypointDataset(Dataset):\n",
        "    def __init__(self, images_dir, labels_dir, transform=None, max_images=None):\n",
        "        self.images_dir = Path(images_dir)\n",
        "        self.labels_dir = Path(labels_dir)\n",
        "        self.image_paths = [p for p in self.images_dir.glob(\"*.jpg\") if (self.labels_dir / f\"{p.stem}.txt\").exists()]\n",
        "        if max_images is not None:\n",
        "            self.image_paths = self.image_paths[:max_images]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path = self.image_paths[idx]\n",
        "        img = cv2.imread(str(image_path))\n",
        "        if img is None:\n",
        "            raise RuntimeError(f\"Failed to load image: {image_path}\")\n",
        "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "        \n",
        "        if self.transform:\n",
        "            img = Image.fromarray(img)\n",
        "            image_tensor = self.transform(img)\n",
        "        \n",
        "        label_path = self.labels_dir / (image_path.stem + \".txt\")\n",
        "        with open(label_path, 'r') as f:\n",
        "            line = f.readline().strip()\n",
        "        parts = line.split()\n",
        "        kp_values = parts[1:]\n",
        "        kp_array = np.array(kp_values, dtype=float).reshape(-1, 3)[:, 0:2]\n",
        "        kp_array *= 224\n",
        "        label_tensor = torch.tensor(kp_array, dtype=torch.float32)\n",
        "        return image_tensor, label_tensor\n",
        "\n",
        "# Focal Loss\n",
        "def weighted_mse_loss(pred, target, alpha=55.0, beta=5.0, threshold=0.1):\n",
        "    \"\"\"\n",
        "    Compute a weighted mean squared error loss.\n",
        "    Pixels with ground truth value greater than 'threshold' are considered foreground and weighted by alpha.\n",
        "    Background pixels are weighted by beta.\n",
        "    \"\"\"\n",
        "    # Create a weight tensor matching the target shape\n",
        "    weights = torch.where(target > threshold,\n",
        "                          torch.tensor(alpha, device=target.device),\n",
        "                          torch.tensor(beta, device=target.device))\n",
        "    loss = weights * (pred - target) ** 2\n",
        "    #loss = (pred-target) ** 2\n",
        "    return loss.mean()\n",
        "\n",
        "# Training Function\n",
        "def train(model, train_loader, val_loader, learning_rate=0.00001, num_epochs=150, device='cuda'):\n",
        "    torch.manual_seed(420)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5)\n",
        "    \n",
        "    H, W = 224, 224\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        running_train_pck = 0.0\n",
        "        train_count = 0\n",
        "        \n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)  # labels: [B, 13, 2]\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            heatmap_outputs = model(images)\n",
        "            heatmap_outputs = F.interpolate(heatmap_outputs, size=(H, W), mode='bilinear', align_corners=False)\n",
        "            # Batch generate ground truth heatmaps (shape: [B, 13, H, W])\n",
        "            gt_heatmaps = generate_heatmaps_batch(labels, H=H, W=W, sigma=4)\n",
        "            \n",
        "            loss = weighted_mse_loss(heatmap_outputs, gt_heatmaps)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() \n",
        "            \n",
        "            # Compute training PCK on this batch\n",
        "            batch_pck = compute_pck(heatmap_outputs, labels)\n",
        "            bsize = images.size(0)\n",
        "            running_train_pck += batch_pck * bsize\n",
        "            train_count += bsize\n",
        "        \n",
        "        avg_train_loss = running_loss / len(train_loader)\n",
        "        avg_train_pck = running_train_pck / train_count\n",
        "        \n",
        "        model.eval()\n",
        "        val_running_loss = 0.0\n",
        "        running_val_pck = 0.0\n",
        "        val_count = 0\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "                heatmap_outputs = model(images)\n",
        "                heatmap_outputs = F.interpolate(heatmap_outputs, size=(H, W), mode='bilinear', align_corners=False)\n",
        "                gt_heatmaps = generate_heatmaps_batch(labels, H=H, W=W, sigma=4)\n",
        "                loss = weighted_mse_loss(heatmap_outputs, gt_heatmaps)\n",
        "                val_running_loss += loss.item()\n",
        "                \n",
        "                batch_pck = compute_pck(heatmap_outputs, labels)\n",
        "                bsize = images.size(0)\n",
        "                running_val_pck += batch_pck * bsize\n",
        "                val_count += bsize\n",
        "                \n",
        "        avg_val_loss = val_running_loss / len(val_loader)\n",
        "        avg_val_pck = running_val_pck / val_count\n",
        "        scheduler.step(avg_val_loss)\n",
        "        \n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}] \"\n",
        "              f\"Train Loss: {avg_train_loss:.4f}, Train PCK: {avg_train_pck*100:.2f}% | \"\n",
        "              f\"Val Loss: {avg_val_loss:.4f}, Val PCK: {avg_val_pck*100:.2f}%\")\n",
        "        \n",
        "        if (epoch + 1) % 10 == 0:\n",
        "            with torch.no_grad():\n",
        "                images, labels = next(iter(val_loader))\n",
        "                images = images.to(device)\n",
        "                pred_heatmaps = model(images)\n",
        "                pred_heatmaps = F.interpolate(pred_heatmaps, size=(H, W), mode='bilinear', align_corners=False)\n",
        "                gt_heatmaps = generate_heatmaps_batch(labels.to(device), H=H, W=W, sigma=4)\n",
        "                plot_heatmaps(images, gt_heatmaps, pred_heatmaps)\n",
        "    \n",
        "    return model\n",
        "\n",
        "def evaluate(model, data_loader, device='cuda', threshold=20):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    total_pck = 0.0\n",
        "    count = 0\n",
        "    H, W = 224, 224\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            outputs = F.interpolate(outputs, size=(H, W), mode='bilinear', align_corners=False)\n",
        "            gt_heatmaps = generate_heatmaps_batch(labels, H=H, W=W, sigma=4)\n",
        "            loss = weighted_mse_loss(outputs, gt_heatmaps)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            batch_pck = compute_pck(outputs, labels, threshold)\n",
        "            total_pck += batch_pck * inputs.size(0)\n",
        "            count += inputs.size(0)\n",
        "    avg_loss = total_loss / count\n",
        "    avg_pck = total_pck / count\n",
        "    print(f'Validation Loss: {avg_loss:.4f}, PCK: {avg_pck*100:.2f}%')\n",
        "    return avg_loss, avg_pck\n",
        "\n",
        "# PCK Evaluation\n",
        "def compute_pck(outputs, labels, threshold=20):\n",
        "    \"\"\"\n",
        "    Compute PCK only for keypoints whose labels are not [0, 0].\n",
        "    outputs: predicted heatmaps used with soft_argmax to get coordinates (shape: [B, 13, H, W])\n",
        "    labels: ground truth keypoints (shape: [B, 13, 2])\n",
        "    threshold: distance threshold (in pixels)\n",
        "    \"\"\"\n",
        "    coords = hard_argmax(outputs)  # shape: [B, 13, 2]\n",
        "    batch_size = coords.shape[0]\n",
        "    labels = labels.view(batch_size, -1, 2)  # [B, 13, 2]\n",
        "    \n",
        "    # Create a mask for valid keypoints (i.e., not both coordinates equal to 0)\n",
        "    valid_mask = ~((labels == 0).all(dim=2))  # shape: [B, 13], True if keypoint is valid\n",
        "    \n",
        "    # Compute Euclidean distances per keypoint\n",
        "    distances = torch.norm(coords - labels, dim=2)  # shape: [B, 13]\n",
        "    \n",
        "    # For each keypoint, if distance < threshold, mark it correct (only if valid)\n",
        "    correct = ((distances < threshold).float() * valid_mask.float())  # shape: [B, 13]\n",
        "    \n",
        "    # For each sample, count number of valid keypoints (avoid division by zero)\n",
        "    valid_counts = valid_mask.sum(dim=1).float()  # shape: [B]\n",
        "    \n",
        "    per_sample_pck = []\n",
        "    for i in range(batch_size):\n",
        "        if valid_counts[i] > 0:\n",
        "            pck = correct[i].sum() / valid_counts[i]\n",
        "        else:\n",
        "            pck = torch.tensor(0.0, device=correct.device)\n",
        "        per_sample_pck.append(pck)\n",
        "    per_sample_pck = torch.stack(per_sample_pck)\n",
        "    return per_sample_pck.mean().item()\n",
        "\n",
        "import numpy as np\n",
        "def hard_argmax(heatmaps):\n",
        "    B, J, H, W = heatmaps.shape\n",
        "    # Flatten each heatmap to shape [B, J, H*W]\n",
        "    flat = heatmaps.view(B, J, -1)\n",
        "    # Get indices of maximum value\n",
        "    indices = flat.argmax(dim=-1)  # shape [B, J]\n",
        "    # Compute x and y coordinates\n",
        "    x = indices % W\n",
        "    y = indices // W\n",
        "    coords = torch.stack((x.float(), y.float()), dim=-1)\n",
        "    return coords\n",
        "def plot_heatmaps(images, gt_heatmaps, pred_heatmaps, num_samples=2):\n",
        "    for i in range(min(num_samples, images.size(0))):\n",
        "        plt.figure(figsize=(15, 5))\n",
        "        plt.subplot(1, 3, 1)\n",
        "        img = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        img = img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])\n",
        "        img = np.clip(img, 0, 1)\n",
        "        plt.imshow(img)\n",
        "        plt.title(\"Input Image\")\n",
        "        plt.subplot(1, 3, 2)\n",
        "        plt.imshow(gt_heatmaps[i, 0].cpu().numpy())\n",
        "        plt.title(\"Ground Truth Heatmap\")\n",
        "        plt.subplot(1, 3, 3)\n",
        "        plt.imshow(pred_heatmaps[i, 0].cpu().numpy())\n",
        "        plt.title(\"Predicted Heatmap\")\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\liufy\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[6], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m model \u001b[38;5;241m=\u001b[39m closedPoseTransformer(backbone\u001b[38;5;241m=\u001b[39mbackbone,transformer\u001b[38;5;241m=\u001b[39mtransformer,decoder\u001b[38;5;241m=\u001b[39mdecoder)\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 31\u001b[0m model \u001b[38;5;241m=\u001b[39m train(model, train_loader, val_loader, learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.005\u001b[39m, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     32\u001b[0m evaluate(model, test_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n",
            "Cell \u001b[1;32mIn[4], line 111\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, val_loader, learning_rate, num_epochs, device)\u001b[0m\n\u001b[0;32m    109\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m    110\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m--> 111\u001b[0m running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem() \n\u001b[0;32m    113\u001b[0m \u001b[38;5;66;03m# Compute training PCK on this batch\u001b[39;00m\n\u001b[0;32m    114\u001b[0m batch_pck \u001b[38;5;241m=\u001b[39m compute_pck(heatmap_outputs, labels)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "np.random.seed(420)\n",
        "torch.manual_seed(420)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if device.type == 'cuda':\n",
        "    torch.cuda.manual_seed_all(420)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "dataset = KeypointDataset('../datasets/train_subset_single/standardized_images',\n",
        "                            '../datasets/train_subset_single/labels',\n",
        "                            transform=transform,\n",
        "                            max_images=2000)\n",
        "\n",
        "train_len = int(0.8 * len(dataset))\n",
        "val_len = int(0.1 * len(dataset))\n",
        "test_len = len(dataset) - train_len - val_len\n",
        "train_set, val_set, test_set = random_split(dataset, [train_len, val_len, test_len])\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=30, shuffle=True)\n",
        "val_loader = DataLoader(val_set, batch_size=30, shuffle=False)\n",
        "test_loader = DataLoader(test_set, batch_size=30, shuffle=False)\n",
        "backbone = ViTBackbone()\n",
        "transformer = Transformer(feature_dim=768)\n",
        "decoder = HeatmapDecoder(feature_dim=768, num_joints=13)\n",
        "model = closedPoseTransformer(backbone=backbone,transformer=transformer,decoder=decoder).cuda()\n",
        "model = model.to(device)\n",
        "model = train(model, train_loader, val_loader, learning_rate=0.005, num_epochs=10, device=device)\n",
        "evaluate(model, test_loader, device=device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
